#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Sep 26 18:58:49 2021

@author: josedelcour
"""
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('~/Downloads/CleanCreditScoring.csv')
df.columns = df.columns.str.lower()

for c in ['income', 'assets', 'debt']:
    df[c] = df[c].replace(to_replace=99999999, value=0)
df['default'] = (df.status == 'bad').astype(int)   
df.drop(columns='status')
y = df['default']

  
from sklearn.compose import make_column_selector as selector

numerical_columns_selector = selector(dtype_exclude=object)
categorical_columns_selector = selector(dtype_include=object)

numerical_columns = numerical_columns_selector(df)
categorical_columns = categorical_columns_selector(df)

from sklearn.model_selection import train_test_split
df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)
df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)
y_train_full = df_train_full.default
y_train=df_train.default
y_val=df_val.default
y_test=df_test.default
del df_train['status']
del df_test['status']
del df_val['status']

from sklearn.metrics import auc
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)

for col in numerical_columns:    
    model.fit(pd.DataFrame(df_train[col]), y_train)
    y_pred = model.predict_proba(pd.DataFrame(df_val[col]))
    fpr, tpr, thresholds = roc_curve(y_val, y_pred[:,1])
    score=auc(fpr, tpr)
    print(col, score)

#Q1:seniority

col=['seniority', 'income', 'assets', 'records', 'job', 'home'] 

numerical_columns_selector = selector(dtype_exclude=object)
categorical_columns_selector = selector(dtype_include=object)

numerical = numerical_columns_selector(df[col])
categorical = categorical_columns_selector(df[col])

df_train_full, df_test, y_train_full, y_test = train_test_split(df[col],y, test_size=0.2, random_state=1)
df_train, df_val, y_train, y_val = train_test_split(df_train_full, y_train_full, test_size=0.25, random_state=1)

dv = DictVectorizer(sparse=False)

train_dict = df_train[categorical + numerical].to_dict(orient='records')
X_train = dv.fit_transform(train_dict)
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)
model.fit(X_train, y_train)

val_dict = df_val[categorical + numerical].to_dict(orient='records')
X_val = dv.transform(val_dict)
y_pred = model.predict_proba(X_val)[:, 1]

fpr, tpr, thresholds = roc_curve(y_val, y_pred)
scoreauc=auc(fpr, tpr)
print( scoreauc)
#Q2: 0.80
# plt.plot(fpr,tpr,[0,1])
score=[]
thresholds = np.linspace(0,1,101)
for t in thresholds:
    actual_positive = (y_val == 1)
    actual_negative = (y_val == 0)
    
    predict_positive = (y_pred >= t)
    predict_negative = (y_pred < t)
    
    tp = (actual_positive & predict_positive).sum()
    tn = (actual_negative & predict_negative).sum()
    
    fp = (predict_positive & actual_negative).sum()
    fn = (predict_negative & actual_positive).sum()
    score.append((t,tp,fp,fn,tn))
    
Score=pd.DataFrame(score)
Score.columns = ['t','tp','fp','fn','tn']
Score.tp/(Score.tp+Score.fp)
Score['tpr'] = Score.tp/(Score.tp+Score.fn)
Score['fpr'] = Score.fp/(Score.fp+Score.tn)
Score['precision'] = Score.tp/(Score.tp+Score.fp)
Score['recall'] = Score.tp/(Score.tp+Score.fn)

#Q3: 0.4  (0.38)
#plt.plot(fpr,tpr,[0,1])


Score['F1'] = 2*Score['precision']*Score['recall']/ (Score['precision'] + Score['recall'])
#Q4: 0.3 (0.33)


def train(df_train, y_train, C=1.0):
    dicts = df_train[categorical + numerical].to_dict(orient='records')

    dv = DictVectorizer(sparse=False)
    X_train = dv.fit_transform(dicts)

    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)
    model.fit(X_train, y_train)
    
    return dv, model

def predict(df, dv, model):
    dicts = df[categorical + numerical].to_dict(orient='records')

    X = dv.transform(dicts)
    y_pred = model.predict_proba(X)[:, 1]

    return y_pred




from sklearn.model_selection import KFold
from tqdm.auto import tqdm

n_splits = 5

#[0.001, 0.01, 0.1, 0.5, 1, 5, 10]:
    
for C in tqdm([0.01,0.1,1,10]):
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)
    scores = []

    for train_idx, val_idx in kfold.split(df_train_full):
        df_train = df_train_full.iloc[train_idx]
        df_val = df_train_full.iloc[val_idx]

        y_train = y_train_full.iloc[train_idx]
        y_val = y_train_full.iloc[val_idx]

        dv, model = train(df_train, y_train, C=C)
        y_pred = predict(df_val, dv, model)

        auc = roc_auc_score(y_val, y_pred)
        scores.append(auc)

    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))

#Q5: 0.023->0.014    
#Q5: C = 10 ???
# TRANSFORM DF_TRAIN_FULL:
from sklearn.model_selection import cross_val_score
dicts = df_train_full[categorical + numerical].to_dict(orient='records')
dv = DictVectorizer(sparse=False)
X = pd.DataFrame(dv.fit_transform(dicts))
scores=[]

y = y_train_full
for  C in [0.01,0.1,1,10]:
    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)
    score = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
    print('C=%s %.3f +- %.3f' % (C, np.mean(score), np.std(score)))