#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Oct 12 07:48:11 2021

@author: josedelcour
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

columns = [
    'neighbourhood_group', 'room_type', 'latitude', 'longitude',
    'minimum_nights', 'number_of_reviews','reviews_per_month',
    'calculated_host_listings_count', 'availability_365',
    'price'
]
df = pd.read_csv('~/Downloads/AB_NYC_2019.csv', usecols=columns)
df.reviews_per_month = df.reviews_per_month.fillna(0)
df["price"] =np.log1p(df.price)

X = df.drop(columns = "price")
y = df["price"]

X_train_full, X_test, y_train_full, y_test = train_test_split(X,y, train_size=0.8, random_state=1)
X_train, X_val, y_train, y_val= train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=1)
                                                 
categorical_col=['neighbourhood_group','room_type']
numerical_col = ['latitude', 'longitude',
    'minimum_nights', 'number_of_reviews','reviews_per_month',
    'calculated_host_listings_count', 'availability_365']

from sklearn.feature_extraction import DictVectorizer

dict_train = X_train.to_dict(orient='records')
dict_val = X_val.to_dict(orient='records')
dict_test = X_test.to_dict(orient='records')

dv = DictVectorizer(sparse=False)

X_train = dv.fit_transform(dict_train)
X_val = dv.transform(dict_val)
X_test = dv.transform(dict_test)

## DECISION TREE CLASSIFIER
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import roc_auc_score

dt = DecisionTreeRegressor(max_depth=1)
dt.fit(X_train, y_train)

y_pred = dt.predict(X_train)
importance = dt.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
#plt.bar([x for x in range(len(importance))], importance)
#plt.show()
##dv. feature_names_
##Q1-> reviews_per_month

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
rf = RandomForestRegressor(n_estimators=10,random_state=1,n_jobs=-1)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_val)
score = mean_squared_error(y_val, y_pred)
print(np.sqrt(score)) 
##Q2: 0.461 ->0.459
"""
scorelist=[]
for rs in range(10,210,10):
    rf = RandomForestRegressor(n_estimators=rs,random_state=1,n_jobs=-1)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_val)
    print(rs, mean_squared_error(y_val, y_pred))
    scorelist.append(np.sqrt( mean_squared_error(y_val, y_pred)))
plt.plot(range(10,210,10), scorelist)
score = mean_squared_error(y_val, y_pred)
##Q3: Resp=10

for ms in range(10,30,5):
    print(f"depth={ms}")
    for rs in range(10,110,10):
        rf = RandomForestRegressor(n_estimators=rs,random_state=1,max_depth=ms, n_jobs=-1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_val)
        rmse= mean_squared_error(y_val, y_pred)
        print(f"{rs} -> {rmse}")
##Q4: Reps depth = 10
"""
## FEATURE IMPORTANCE
rf = RandomForestRegressor(n_estimators=10,random_state=1,max_depth = 20, n_jobs=-1)
rf.fit(X_train, y_train)
results = pd.DataFrame(rf.feature_importances_)
results = results.T
results.columns = dv.feature_names_
results *=100
print(results.T)
##Q5:->  room_type=Entire home/apt

##XGBOOST ----------------------------------------------------------
import xgboost as xgb
xgb_params = {
    'eta': 0.3, 
    'max_depth': 6,
    'min_child_weight': 1,
    
    'objective': 'reg:squarederror',
    'nthread': 8,
    
    'seed': 1,
    'verbosity': 1,
}
dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=dv.feature_names_)
dval = xgb.DMatrix(X_val, label=y_val, feature_names=dv.feature_names_)

y_pred = model.predict(dval)
watchlist = [(dtrain, 'train'), (dval, 'val')]
model = xgb.train(xgb_params, dtrain, num_boost_round=100, evals=watchlist)
# eta=0.3 - rmse = 0.43621
# eta=0.1 - rmse = 0.43250
#eta=0.01 - rmse = 1.63045

##Q6: eta=0.1